{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we're working with this time is less structured. We only have 2 columns in total. One indicating whether or not it is SPAM, and one containing the message. Let's do a bit more exploration on the dataset this time.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do I want to know?\n",
    "1. Number of unique words\n",
    "2. List of the most common words\n",
    "3. How many email examples do we have in total?\n",
    "3. SPAM/HAM ratio of the emails\n",
    "4. Maximum, Minimum, and Average length of emails\n",
    "5. List of most common words in emails labled SPAM\n",
    "6. List of most common words in emails labled HAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing relevant models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from spam_classification_utils import *\n",
    "import re\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset. It is always good practice to make a copy of the dataframe before you make any changes so you always have the original. \n",
    "df = pd.read_csv('Datasets/spam_data.csv', encoding='utf-8')\n",
    "df_orig = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by cleaning up the data a little, then going through the things we want to know one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import re\n",
    "df = df_orig\n",
    "df['Message'] = df['Message'].apply(clean_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = df['Message'].str.split(expand=True).stack().value_counts()\n",
    "unique_words = unique_words.reset_index()\n",
    "unique_words.columns = ['word', 'count']\n",
    "\n",
    "print('There is a total of ' + str(df.shape[0]) + ' emails in the dataset')\n",
    "print()\n",
    "print('There is a split of ' + str(df['Category'].value_counts().iloc[0]) + ' SPAM emails and ' + str(df['Category'].value_counts().iloc[1]) + ' HAM emails')\n",
    "print()\n",
    "print('The longest email has a length of ' + str(df['Message'].str.split().str.len().max()) + ' and the shortest email has length of ' + str(df['Message'].str.split().str.len().min()) +'. The average length is ' + str(int(df['Message'].str.split().str.len().mean())))\n",
    "print()\n",
    "print('There are a total of ' + str(len(unique_words)) + ' unique words in the dataset')\n",
    "print()\n",
    "print('The most commonly appearing words are:')\n",
    "display(unique_words[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one would expect, pretty much all of the most common words are stop words like 'to', 'you', 'I', 'my', 'is', 'and' etc. These appear in almost every email so it makes sense for them to be the most common, but we are also not getting anything meaningful from them, so its ideal to remove them when we try to build our model. Lets remove the stop words, then take a closer look specifically at the SPAM and HAM emails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = pd.read_json('stopwords-en.json')\n",
    "stop_words = stop_words[0].tolist()\n",
    "filtered_unique_words = unique_words[~unique_words['word'].isin(stop_words)]\n",
    "display(filtered_unique_words[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_emails = df[df['Category'] == 'spam']\n",
    "ham_emails = df[df['Category'] == 'ham']\n",
    "spam_unique_words = spam_emails['Message'].str.split(expand=True).stack().value_counts()\n",
    "ham_unique_words = ham_emails['Message'].str.split(expand=True).stack().value_counts()\n",
    "\n",
    "spam_unique_words = spam_unique_words.reset_index()\n",
    "spam_unique_words.columns = ['word', 'count']\n",
    "\n",
    "ham_unique_words = ham_unique_words.reset_index()\n",
    "ham_unique_words.columns = ['word', 'count']\n",
    "\n",
    "filtered_spam_unique_words = spam_unique_words[~spam_unique_words['word'].isin(stop_words)]\n",
    "filtered_ham_unique_words = ham_unique_words[~ham_unique_words['word'].isin(stop_words)]\n",
    "\n",
    "print('The most commonly appearing words in SPAM emails are:')\n",
    "display(filtered_spam_unique_words[0:10])\n",
    "print()\n",
    "print('The most commonly appearing words in HAM emails are:')\n",
    "display(filtered_ham_unique_words[0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already we can see the most commonly appearing words between spam and ham emails differ greatly. Now that we have a general idea of whats in the datasets, we can try to learn a basic model using our previous somewhat naive method that breaks down the emails into the common words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_list = filtered_unique_words['word'].tolist()\n",
    "unique_words_truncated = unique_words_list[0:3000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = np.zeros((len(df), len(unique_words_truncated)), dtype=int)\n",
    "\n",
    "for i, word in enumerate(unique_words_truncated):\n",
    "    word_counts[:, i] = df['Message'].apply(lambda msg: count_word_in_message(word, msg))\n",
    "    print(str(i) if i%10 == 0 else '')\n",
    "word_counts_df = pd.DataFrame(word_counts, columns=unique_words_truncated)\n",
    "\n",
    "# Concatenate the word counts DataFrame with the original DataFrame\n",
    "result_df = pd.concat([word_counts_df, df[['Category']]], axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "word_columns = result_df.columns[0:-1]\n",
    "label_column = result_df.columns[-1]\n",
    "result_df[word_columns] = (result_df[word_columns] - result_df[word_columns].min()) / (result_df[word_columns].max() - result_df[word_columns].min())\n",
    "result_df['Category'] = label_encoder.fit_transform(result_df['Category'])\n",
    "# check our mapping to see if the encoder correctly labels ham 0 and spam 1\n",
    "print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(result_df[word_columns], result_df[label_column], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "linear_model = Sequential()\n",
    "linear_model.add(Dense(128, activation = 'relu', input_shape = (3000,)))\n",
    "linear_model.add(Dense(64, activation='relu'))\n",
    "linear_model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "linear_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "linear_model.fit(X_train, y_train.T, batch_size=10,\n",
    "          epochs=10, validation_split=0.1)\n",
    "\n",
    "y_pred_prob = linear_model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the model: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "honestly this is already pretty good maybe i just stop here lmao. \n",
    "Ok that was a joke. Let's make an RNN using the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['Category'] = label_encoder.fit_transform(df['Category'])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words = len(unique_words_list))\n",
    "tokenizer.fit_on_texts(df['Message'])\n",
    "sequences = tokenizer.texts_to_sequences(df['Message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(len(seq) for seq in sequences)  # Or choose a fixed length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(padded_sequences)\n",
    "print(padded_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(padded_sequences, df['Category'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_train.shape)\n",
    "display(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, f1_score, recall_score, precision_score\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "vocab_size = len(unique_words_list)\n",
    "max_length = 171\n",
    "METRICS = ['accuracy', 'Precision', 'Recall']\n",
    "      \n",
    "\n",
    "\n",
    "simple_RNN_model = models.Sequential()\n",
    "simple_RNN_model.add(layers.Embedding(input_dim = vocab_size, output_dim=64))\n",
    "simple_RNN_model.add(layers.Bidirectional(layers.LSTM(128, recurrent_dropout=0.2)))\n",
    "simple_RNN_model.add(layers.Dense(1, activation='sigmoid'))  # Use 'softmax' if you have multiple classes\n",
    "\n",
    "simple_RNN_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=METRICS)  # Adjust loss based on the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the log directory for TensorBoard\n",
    "file_name = 'test3'\n",
    "tensorboard = TensorBoard(log_dir=\"logs\\\\{}\".format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val), callbacks = [tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = simple_RNN_model.predict(X_val)\n",
    "y_pred = (y_pred_prob > 0.99).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuracy of the model: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have very slightly improved our performance! 98.92% accuracy on our test set compared to 98.02% is pretty good for our relatively basic RNN model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 50 # the length of all sequences (number of words per sample)\n",
    "EMBEDDING_SIZE = 100  # Using 100-Dimensional GloVe embedding vectors\n",
    "TEST_SIZE = 0.25 # ratio of testing set\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20 # number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "vocab_size = len(unique_words_list)\n",
    "max_length = 171\n",
    "\n",
    "\n",
    "adv_RNN_model = models.Sequential()\n",
    "adv_RNN_model.add(layers.Embedding(input_dim=vocab_size, output_dim=64))\n",
    "adv_RNN_model.add(layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))  # Set return_sequences=True\n",
    "adv_RNN_model.add(layers.BatchNormalization())\n",
    "\n",
    "adv_RNN_model.add(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=False))  # Last LSTM can return sequences=False\n",
    "adv_RNN_model.add(layers.BatchNormalization())\n",
    "\n",
    "adv_RNN_model.add(layers.Dense(1, activation='sigmoid'))  # For binary classification\n",
    "\n",
    "adv_RNN_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "adv_RNN_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "adv_RNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adv_RNN_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Confusion matrix\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m (\u001b[43madv_RNN_model\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m plot_confusion_matrix(confusion_matrix(y_test, y_pred), class_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHam\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpam\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest data confusion matrix with classification threshold at 0.77\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'adv_RNN_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "y_pred = (adv_RNN_model.predict(X_test) > 0.78).astype(\"int32\")\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y_test, y_pred), class_names=['Ham','Spam'])\n",
    "plt.title('Test data confusion matrix with classification threshold at 0.77')\n",
    "plt.savefig('images/confusion77.png',bbox_inches='tight',dpi=400, pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = adv_RNN_model.predict(X_val)\n",
    "y_pred = (y_pred_prob > 0.99).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuracy of the model: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
