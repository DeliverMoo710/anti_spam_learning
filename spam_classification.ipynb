{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details about the dataset:\n",
    "\n",
    "The csv file contains 5172 rows, each row for each email. There are 3002 columns. The first column indicates Email name. The name has been set with numbers and not recipients' name to protect privacy. The last column has the labels for prediction : 1 for spam, 0 for not spam. The remaining 3000 columns are the 3000 most common words in all the emails, after excluding the non-alphabetical characters/words. For each row, the count of each word(column) in that email(row) is stored in the respective cells. Thus, information regarding all 5172 emails are stored in a compact dataframe rather than as separate text files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start this by exploring our dataset a little bit. The most important thing to consider is that:\n",
    "1. the label for each email is stored in the very last column\n",
    "2. the columns are not of the email itself but rather the count for each of the most common 3000 words in all emails. \n",
    "3. A total of 5172 email hence 5172 rows\n",
    "4. A total of 3000 most common words, with the first column being the email id, and the last being a label for a total of 3002 columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are things I want to know? \n",
    "1. spam to non-spam ratio\n",
    "2. most commonly appearing word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try building multiple models and see how they do.\n",
    "1. Linear Regression\n",
    "2. Feed Forward Neural Network\n",
    "3. Bayesian Network Model\n",
    "4. Skip connection\n",
    "5. gradient clipping\n",
    "\n",
    "Perhaps in the future we can find a datasset where the actual emails are maintained so we can try building a recurrent neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import related libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv and convert it into a dataframe. We than make a copy of the dataframe before we make any changes to keep an original copy\n",
    "df = pd.read_csv('Datasets/emails.csv')\n",
    "df_orig = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve a list of the 3000 most common words we chose to use\n",
    "common_words = df.columns.tolist()[1:-1]\n",
    "\n",
    "# save the number of training examples and number of words\n",
    "m_train = df.shape[0]\n",
    "num_words = len(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email No.     0\n",
      "the           0\n",
      "to            0\n",
      "ect           0\n",
      "and           0\n",
      "             ..\n",
      "military      0\n",
      "allowing      0\n",
      "ff            0\n",
      "dry           0\n",
      "Prediction    0\n",
      "Length: 3002, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# fill missing values if there are any\n",
    "print(df.isnull().sum())\n",
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data normalisation\n",
    "word_columns = df.columns[1:-1]\n",
    "label_column = df.columns[-1]\n",
    "df[word_columns] = (df[word_columns] - df[word_columns].min()) / (df[word_columns].max() - df[word_columns].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\n",
      "0    3672\n",
      "1    1500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_column = df.columns[-1]\n",
    "spam_counts = df[label_column].value_counts()\n",
    "print(spam_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[word_columns], df[label_column], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is (4137, 3000)\n",
      "The shape of y_train is (4137,)\n",
      "The shape of X_test is (1035, 3000)\n",
      "The shape of y_test is (1035,)\n"
     ]
    }
   ],
   "source": [
    "print('The shape of X_train is ' + str(X_train.shape))\n",
    "print('The shape of y_train is ' + str(y_train.shape))\n",
    "print('The shape of X_test is ' + str(X_test.shape))\n",
    "print('The shape of y_test is ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of X_train is (4137, 3000)\n",
    "The shape of y_train is (4137,)\n",
    "The shape of X_test is (1035, 3000)\n",
    "The shape of y_test is (1035,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with training a basic linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8484 - loss: 0.3255 - val_accuracy: 0.9783 - val_loss: 0.1033\n",
      "Epoch 2/10\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9861 - loss: 0.0506 - val_accuracy: 0.9783 - val_loss: 0.0876\n",
      "Epoch 3/10\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9942 - loss: 0.0199 - val_accuracy: 0.9807 - val_loss: 0.0476\n",
      "Epoch 4/10\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9975 - loss: 0.0099 - val_accuracy: 0.9807 - val_loss: 0.0512\n",
      "Epoch 5/10\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9996 - loss: 0.0051 - val_accuracy: 0.9855 - val_loss: 0.0544\n",
      "Epoch 6/10\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9999 - loss: 0.0034 - val_accuracy: 0.9831 - val_loss: 0.0735\n",
      "Epoch 7/10\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9996 - loss: 0.0032 - val_accuracy: 0.9831 - val_loss: 0.0674\n",
      "Epoch 8/10\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9999 - loss: 0.0011 - val_accuracy: 0.9807 - val_loss: 0.0755\n",
      "Epoch 9/10\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9999 - loss: 0.0012 - val_accuracy: 0.9783 - val_loss: 0.0979\n",
      "Epoch 10/10\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9991 - loss: 0.0039 - val_accuracy: 0.9758 - val_loss: 0.1202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2097c31f1d0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model = Sequential()\n",
    "linear_model.add(Dense(128, activation = 'relu', input_shape = (3000,)))\n",
    "linear_model.add(Dense(64, activation='relu'))\n",
    "linear_model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "linear_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "linear_model.fit(X_train, y_train.T, batch_size=10,\n",
    "          epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy: 0.9991 - loss: 0.0039 - val_accuracy: 0.9758 - val_loss: 0.1202\n",
    "\n",
    "We seem to be performing relatively well on the training set. Let's check our accuracy on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Accuracy of the model: 0.9749\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = linear_model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the model: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of the model: 0.9749\n",
    "\n",
    "Seems like we are performing fine on the test set too. This is a great start! Let's try implementing a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "ff_model = Sequential()\n",
    "ff_model.add(Dense(128, activation = 'relu', input_shape = (3000,)))\n",
    "ff_model.add(Dense(64, activation='relu'))\n",
    "ff_model.add(Dense(32, activation='relu'))\n",
    "ff_model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "ff_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'test4'\n",
    "tensorboard = TensorBoard(log_dir=\"logs\\\\{}\".format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8414 - loss: 0.3408 - val_accuracy: 0.9758 - val_loss: 0.0723\n",
      "Epoch 2/10\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9895 - loss: 0.0332 - val_accuracy: 0.9855 - val_loss: 0.0432\n",
      "Epoch 3/10\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9904 - loss: 0.0275 - val_accuracy: 0.9807 - val_loss: 0.0452\n",
      "Epoch 4/10\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9987 - loss: 0.0051 - val_accuracy: 0.9831 - val_loss: 0.0539\n",
      "Epoch 5/10\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9995 - loss: 0.0029 - val_accuracy: 0.9589 - val_loss: 0.3399\n",
      "Epoch 6/10\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9962 - loss: 0.0184 - val_accuracy: 0.9758 - val_loss: 0.0582\n",
      "Epoch 7/10\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9981 - loss: 0.0059 - val_accuracy: 0.9807 - val_loss: 0.0526\n",
      "Epoch 8/10\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9997 - loss: 0.0020 - val_accuracy: 0.9831 - val_loss: 0.0585\n",
      "Epoch 9/10\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9999 - loss: 6.2231e-04 - val_accuracy: 0.9807 - val_loss: 0.0722\n",
      "Epoch 10/10\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.4998e-04 - val_accuracy: 0.9783 - val_loss: 0.0924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x200ae7fa300>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_model.fit(X_train, y_train.T, batch_size=10,\n",
    "          epochs=10, validation_split=0.1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Accuracy of the model: 0.9787\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = ff_model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the model: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to perform pretty similarily as a simple regression model. Which makes sense since they do have pretty similar structues. Let's try a skip-connection network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6501 - loss: 0.7484 - val_accuracy: 0.7089 - val_loss: 0.5494\n",
      "Epoch 2/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8804 - loss: 0.2905 - val_accuracy: 0.7053 - val_loss: 0.5481\n",
      "Epoch 3/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9346 - loss: 0.1731 - val_accuracy: 0.7754 - val_loss: 0.3641\n",
      "Epoch 4/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9560 - loss: 0.1166 - val_accuracy: 0.9263 - val_loss: 0.1663\n",
      "Epoch 5/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9741 - loss: 0.0766 - val_accuracy: 0.9758 - val_loss: 0.0708\n",
      "Epoch 6/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9789 - loss: 0.0741 - val_accuracy: 0.9674 - val_loss: 0.0701\n",
      "Epoch 7/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9810 - loss: 0.0607 - val_accuracy: 0.9783 - val_loss: 0.0511\n",
      "Epoch 8/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9807 - loss: 0.0613 - val_accuracy: 0.9807 - val_loss: 0.0511\n",
      "Epoch 9/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9809 - loss: 0.0626 - val_accuracy: 0.9783 - val_loss: 0.0508\n",
      "Epoch 10/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9862 - loss: 0.0467 - val_accuracy: 0.9734 - val_loss: 0.0567\n",
      "Epoch 11/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9914 - loss: 0.0286 - val_accuracy: 0.9758 - val_loss: 0.0552\n",
      "Epoch 12/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9863 - loss: 0.0496 - val_accuracy: 0.9783 - val_loss: 0.0552\n",
      "Epoch 13/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9924 - loss: 0.0250 - val_accuracy: 0.9795 - val_loss: 0.0561\n",
      "Epoch 14/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9922 - loss: 0.0274 - val_accuracy: 0.9783 - val_loss: 0.0528\n",
      "Epoch 15/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9891 - loss: 0.0343 - val_accuracy: 0.9734 - val_loss: 0.0627\n",
      "Epoch 16/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9948 - loss: 0.0177 - val_accuracy: 0.9783 - val_loss: 0.0535\n",
      "Epoch 17/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9911 - loss: 0.0246 - val_accuracy: 0.9783 - val_loss: 0.0615\n",
      "Epoch 18/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9977 - loss: 0.0126 - val_accuracy: 0.9758 - val_loss: 0.0630\n",
      "Epoch 19/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9911 - loss: 0.0371 - val_accuracy: 0.9783 - val_loss: 0.0721\n",
      "Epoch 20/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9960 - loss: 0.0179 - val_accuracy: 0.9795 - val_loss: 0.0626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2090dec1ee0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Add, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "inputs = Input(shape=(3000,))\n",
    "\n",
    "# Layer 1\n",
    "x1 = Dense(256, activation='relu')(inputs)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Dropout(0.5)(x1)\n",
    "\n",
    "# Layer 2\n",
    "x2 = Dense(128, activation='relu')(x1)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Dropout(0.5)(x2)\n",
    "\n",
    "# Skip connection to Layer 3\n",
    "x3 = Dense(128, activation='relu')(x2)  # Skip connection from layer 1\n",
    "\n",
    "# Combine Layer 2 and Layer 3 outputs\n",
    "combined = Add()([x2, x3])\n",
    "\n",
    "x4 = Dense(64, activation='relu')(combined)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Dropout(0.5)(x4)\n",
    "\n",
    "x5 = Dense(64, activation='relu')(x4)\n",
    "\n",
    "combined2 = Add()([x4, x5])\n",
    "\n",
    "x6 = Dense(32, activation='relu')(combined)\n",
    "x6 = BatchNormalization()(x6)\n",
    "x6 = Dropout(0.5)(x6)\n",
    "\n",
    "x7 = Dense(32, activation='relu')(x6)\n",
    "\n",
    "combined3 = Add()([x6, x7])\n",
    "# Output layer\n",
    "outputs = Dense(1, activation='sigmoid')(combined3)\n",
    "\n",
    "# Create the model\n",
    "sc_model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "sc_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "sc_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Accuracy of the model: 0.9787\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = sc_model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.8).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the model: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we'll try implementing a naive bayse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9391304347826087\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96       739\n",
      "           1       0.91      0.88      0.89       296\n",
      "\n",
      "    accuracy                           0.94      1035\n",
      "   macro avg       0.93      0.92      0.92      1035\n",
      "weighted avg       0.94      0.94      0.94      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is enough for now, we'll move on to constructing a network useful for detecting a full email. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
